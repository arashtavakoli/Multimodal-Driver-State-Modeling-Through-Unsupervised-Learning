{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "valued-pressing",
   "metadata": {},
   "source": [
    "# Multimodal Driver State Modeling through Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "committed-waterproof",
   "metadata": {},
   "source": [
    "This repository is the code accompanying the paper with the same name. The data for this study is based on the HARMONY dataset. A sample of the data is available at https://osf.io/zextd/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upper-retro",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------\n",
    "#all packages\n",
    "#-----------------------------------------------------------\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import wasserstein_distance\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "import scipy\n",
    "from scipy import stats\n",
    "from scipy import signal\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#All needed packages\n",
    "from scipy.signal import find_peaks\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "import matplotlib.pyplot as plt\n",
    "import re, nltk, spacy, gensim\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from scipy.stats import kstest\n",
    "from scipy.stats import chi2_contingency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "short-payment",
   "metadata": {},
   "source": [
    "###  Data from long trip for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thermal-spread",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------\n",
    "#2.1 Reading all sources of wearable, vehicle, and gaze data\n",
    "#-----------------------------------------------------------\n",
    "\n",
    "#read car's data\n",
    "#reading acc,gyro, and linear acc data, put them together, resample at 100 hz, fill nans and make it a clean dataframe\n",
    "\n",
    "sensors = [\"GyroscopeDatum\",\"AccelerometerDatum\",\"LinearAccelerationDatum\"]\n",
    "df_ultimate = []\n",
    "\n",
    "for sensor in sensors:\n",
    "    df_temp = pd.read_csv(\"C:/Users/Arsalan/Desktop/CVILLE - DC Trips/both hands and car - DC - CVille/raw/car/\"+\"Smartwatch_\"+sensor+\".csv\",parse_dates=['Timestamp'])\n",
    "    #start_date = \"2020-08-21T23:30:59.154+0000\"\n",
    "    #end_date = \"2020-08-22T02:14:56.307+0000\"\n",
    "    start_date = \"2020-08-23T15:16:11.000+0000\"\n",
    "    end_date = \"2020-08-23T17:16:00.000+0000\"\n",
    "    mask = (df_temp['Timestamp'] > start_date) & (df_temp['Timestamp'] <= end_date)\n",
    "    df_temp = df_temp.loc[mask]\n",
    "    if sensor in [\"GyroscopeDatum\",\"AccelerometerDatum\",\"LinearAccelerationDatum\"]:\n",
    "        df_temp.rename(columns={'X': 'X'+sensor[:4], 'Y': 'Y'+sensor[:4],'Z':'Z'+sensor[:4]}, inplace=True)\n",
    "    df_ultimate.append(df_temp)\n",
    "\n",
    "    \n",
    "df_final = df_ultimate[0]\n",
    "for df in df_ultimate[1:]:\n",
    "    df = df.drop(columns=[\"participantid\"])\n",
    "    df.reset_index(drop=True,inplace=True)\n",
    "    df_final = df_final.merge(df,on=['Timestamp',\n",
    "                                         'DeviceId'], how='outer')\n",
    "    \n",
    "    \n",
    "df_final = df_final.set_index(\"Timestamp\")\n",
    "df_final = df_final.groupby([\"DeviceId\"]).resample('100L').mean().ffill().bfill()\n",
    "df_final = df_final.reset_index()\n",
    "df_final[\"magGyro\"] = (df_final[\"XGyro\"]**2 + df_final[\"YGyro\"]**2 + df_final[\"ZGyro\"]**2)**0.5\n",
    "df_final[\"magAcce\"] = (df_final[\"XAcce\"]**2 + df_final[\"YAcce\"]**2 + df_final[\"ZAcce\"]**2)**0.5\n",
    "df_final[\"magLine\"] = (df_final[\"XLine\"]**2 + df_final[\"YLine\"]**2 + df_final[\"ZLine\"]**2)**0.5\n",
    "\n",
    "#keep a copy to not rerun things\n",
    "imu_data_car = df_final.copy()\n",
    "imu_data_car = imu_data_car.set_index(\"Timestamp\")\n",
    "imu_data_car = df_final.copy()\n",
    "imu_data_car = imu_data_car.set_index(\"Timestamp\")\n",
    "imu_data_car = imu_data_car[1:]\n",
    "\n",
    "#2.2 Reading all sources of wearable, vehicle, and gaze data\n",
    "#reading acc,gyro, and linear acc data, put them together, resample at 100 hz, fill nans and make it a clean dataframe\n",
    "\n",
    "\n",
    "sensors = [\"HeartRateDatum\",\"GyroscopeDatum\",\"AccelerometerDatum\",\"LinearAccelerationDatum\",\"Light\"]\n",
    "df_ultimate = []\n",
    "\n",
    "for sensor in sensors:\n",
    "    df_temp = pd.read_csv(\"C:/Users/Arsalan/Desktop/CVILLE - DC Trips/both hands and car - DC - CVille/raw/\"+\"Smartwatch_\"+sensor+\".csv\",parse_dates=['Timestamp'])\n",
    "#     start_date = \"2020-08-21T23:30:59.154+0000\"\n",
    "#     end_date = \"2020-08-22T02:14:56.307+0000\"\n",
    "    start_date = \"2020-08-23T15:16:11.000+0000\"\n",
    "    end_date = \"2020-08-23T17:16:00.000+0000\"\n",
    "    mask = (df_temp['Timestamp'] > start_date) & (df_temp['Timestamp'] <= end_date)\n",
    "    df_temp = df_temp.loc[mask]\n",
    "    if sensor in [\"GyroscopeDatum\",\"AccelerometerDatum\",\"LinearAccelerationDatum\"]:\n",
    "        df_temp.rename(columns={'X': 'X'+sensor[:4]+\"_driver\", 'Y': 'Y'+sensor[:4]+\"_driver\",'Z':'Z'+sensor[:4]+\"_driver\"}, inplace=True)\n",
    "    df_ultimate.append(df_temp)\n",
    "\n",
    "    \n",
    "df_final = df_ultimate[0]\n",
    "for df in df_ultimate[1:]:\n",
    "    df = df.drop(columns=[\"participantid\"])\n",
    "    df.reset_index(drop=True,inplace=True)\n",
    "    df_final = df_final.merge(df,on=['Timestamp',\n",
    "                                         'DeviceId'], how='outer')\n",
    "    \n",
    "    \n",
    "df_final = df_final.set_index(\"Timestamp\")\n",
    "df_final = df_final.groupby([\"DeviceId\"]).resample('100L').mean().ffill().bfill()\n",
    "df_final = df_final.reset_index()\n",
    "df_final[\"magGyro_driver\"] = (df_final[\"XGyro_driver\"]**2 + df_final[\"YGyro_driver\"]**2 + df_final[\"ZGyro_driver\"]**2)**0.5\n",
    "df_final[\"magAcce_driver\"] = (df_final[\"XAcce_driver\"]**2 + df_final[\"YAcce_driver\"]**2 + df_final[\"ZAcce_driver\"]**2)**0.5\n",
    "df_final[\"magLine_driver\"] = (df_final[\"XLine_driver\"]**2 + df_final[\"YLine_driver\"]**2 + df_final[\"ZLine_driver\"]**2)**0.5\n",
    "\n",
    "#keep a copy to not rerun things\n",
    "hr_imu_data = df_final.copy()\n",
    "hr_imu_data = hr_imu_data.set_index(\"Timestamp\")\n",
    "hr_imu_data_left=hr_imu_data[hr_imu_data[\"DeviceId\"]==\"f60691a313420a4e\"]\n",
    "hr_imu_data_right=hr_imu_data[hr_imu_data[\"DeviceId\"]==\"39ca51c16b9ec429\"]\n",
    "hr_imu_data_left.columns = [str(col) + '_left' for col in hr_imu_data_left.columns]\n",
    "#imu_data_right = imu_data_right.set_index(\"Timestamp\")\n",
    "#imu_data_left = imu_data_left.set_index(\"Timestamp\")\n",
    "#2.3 Reading all sources of wearable, vehicle, and gaze data\n",
    "#reading gaze data\n",
    "\n",
    "cols = [ \" AU01_r\",\" AU02_r\",\" AU04_r\",\" AU05_r\",\" AU06_r\",\" AU07_r\",\" AU09_r\",\" AU10_r\",\" AU12_r\",\" AU14_r\",\" AU15_r\",\" AU17_r\",\" AU20_r\",\" AU23_r\",\" AU25_r\",\" AU26_r\",\" AU45_r\",\" AU01_c\",\" AU02_c\",\" AU04_c\",\" AU05_c\",\" AU06_c\",\" AU07_c\",\" AU09_c\",\" AU10_c\",\" AU12_c\",\" AU14_c\",\" AU15_c\",\" AU17_c\",\" AU20_c\",\" AU23_c\",\" AU25_c\",\" AU26_c\",\" AU28_c\", \" AU45_c\"\n",
    ",\" timestamp\",\" confidence\",\" success\",\" gaze_angle_x\",\" gaze_angle_y\",\" p_scale\",\" p_rx\",\" p_ry\",\" p_rz\",\" p_tx\",\" p_ty\",\"video_name\",\"day\" ]\n",
    "gaze = pd.read_csv(\"C:/Users/Arsalan/Desktop/CVILLE - DC Trips/both hands and car - DC - CVille/raw/final_gaze.csv\",usecols=cols)\n",
    "\n",
    "gaze[\"time_start\"] = (gaze[\"video_name\"].str[:-7])\n",
    "gaze[\"time_start\"] = pd.to_datetime(gaze[\"time_start\"],format = '%Y%m%d_%H%M%S')\n",
    "gaze[\" timestamp\"] = pd.to_timedelta(gaze[\" timestamp\"],unit = 's')\n",
    "gaze[\"Timestamp\"] = gaze[\"time_start\"] + gaze[\" timestamp\"]\n",
    "gaze[\"mag_gaze\"] = (gaze[\" gaze_angle_x\"]**2 + gaze[\" gaze_angle_y\"]**2)**0.5\n",
    "gaze[\"Timestamp\"] = gaze[\"Timestamp\"] + pd.Timedelta(value=+4,unit='h')\n",
    "# start_date = pd.Timestamp(\"2020-08-21T23:30:59.154+0000\",tz=None)\n",
    "# end_date = pd.Timestamp(\"2020-08-22T02:14:56.307+0000\",tz=None)\n",
    "start_date = pd.Timestamp(\"2020-08-23T15:16:11.000+0000\",tz=None)\n",
    "end_date = pd.Timestamp(\"2020-08-23T17:16:00.000+0000\",tz=None)\n",
    "start_date = start_date.tz_localize(None)\n",
    "end_date = end_date.tz_localize(None)\n",
    "gaze[\"Timestamp\"] = pd.to_datetime(gaze[\"Timestamp\"])  \n",
    "gaze[\"Timestamp\"] = gaze[\"Timestamp\"].dt.tz_localize(None)\n",
    "mask = (gaze[\"Timestamp\"] > start_date) & (gaze[\"Timestamp\"] <= end_date)\n",
    "gaze = gaze.loc[mask]\n",
    "gaze = gaze.set_index(\"Timestamp\")\n",
    "gaze = gaze[gaze[\" success\"] == 1]\n",
    "gaze = gaze[gaze[\" confidence\"]>0.85]\n",
    "gaze = gaze.resample('100L').mean().bfill().ffill()\n",
    "gaze = gaze.drop(columns=['day'])\n",
    "\n",
    "#2.4 getting all the outside environment data\n",
    "\n",
    "\n",
    "cols = [\"person\",\"bicycle\",\"car\",\"motorcycle\",\"bus\",\"truck\",\"frame\",\"follow_dist\",\"follow_obj\",\"lead_pres\",\"video_name\",\"day\" ]\n",
    "mask_rcnn = pd.read_csv(\"D:/Google Drive/UVA_PhD/Projects/Wearable Projects/Codes/Wearable_Event_detection/csv files/final_mask_rcnn.csv\",usecols=cols)\n",
    "\n",
    "mask_rcnn[\"time_start\"] = (mask_rcnn[\"video_name\"].str[:-16])\n",
    "mask_rcnn[\"time_start\"] = pd.to_datetime(mask_rcnn[\"time_start\"],format = '%Y%m%d_%H%M%S')\n",
    "mask_rcnn[\" timestamp\"] = mask_rcnn[\"frame\"]*0.0333\n",
    "mask_rcnn[\" timestamp\"] = pd.to_timedelta(mask_rcnn[\" timestamp\"],unit = 's')\n",
    "mask_rcnn[\"Timestamp\"] = mask_rcnn[\"time_start\"] + mask_rcnn[\" timestamp\"]\n",
    "mask_rcnn[\"Timestamp\"] = mask_rcnn[\"Timestamp\"] + pd.Timedelta(value=+4,unit='h')\n",
    "# start_date = pd.Timestamp(\"2020-08-21T23:30:59.154+0000\",tz=None)\n",
    "# end_date = pd.Timestamp(\"2020-08-22T02:14:56.307+0000\",tz=None)\n",
    "start_date = pd.Timestamp(\"2020-08-23T15:16:11.000+0000\",tz=None)\n",
    "end_date = pd.Timestamp(\"2020-08-23T17:16:00.000+0000\",tz=None)\n",
    "start_date = start_date.tz_localize(None)\n",
    "end_date = end_date.tz_localize(None)\n",
    "mask_rcnn[\"Timestamp\"] = pd.to_datetime(mask_rcnn[\"Timestamp\"])  \n",
    "mask_rcnn[\"Timestamp\"] = mask_rcnn[\"Timestamp\"].dt.tz_localize(None)\n",
    "mask = (mask_rcnn[\"Timestamp\"] > start_date) & (mask_rcnn[\"Timestamp\"] <= end_date)\n",
    "mask_rcnn = mask_rcnn.loc[mask]\n",
    "mask_rcnn = mask_rcnn.set_index(\"Timestamp\")\n",
    "mask_rcnn[\"lead_pres\"] = mask_rcnn[\"lead_pres\"].fillna(0)\n",
    "mask_rcnn[\"follow_obj\"] = mask_rcnn[\"follow_obj\"].fillna(15)\n",
    "mask_rcnn[\"follow_dist\"] = mask_rcnn[\"follow_dist\"].fillna(5000)\n",
    "mask_rcnn = mask_rcnn.resample('100L').bfill().ffill()\n",
    "\n",
    "#3. put them all together\n",
    "\n",
    "hr_imu_data_right.index = hr_imu_data_right.index.tz_localize(None)\n",
    "hr_imu_data_left.index = hr_imu_data_left.index.tz_localize(None)\n",
    "imu_data_car.index = imu_data_car.index.tz_localize(None)\n",
    "total_data = pd.concat([mask_rcnn,gaze,hr_imu_data_right,hr_imu_data_left,imu_data_car],axis=1)\n",
    "\n",
    "#4. combining them with change points - copying bcp groups from previous steps\n",
    "total_data = total_data[:-1]\n",
    "#total_data[\"bcp_groups\"] = list_groups[\"new_group\"].values\n",
    "#total_data = total_data.dropna()\n",
    "total_data[\"gaze_std_x\"]=total_data[\" gaze_angle_x\"].rolling(10,min_periods=0).std()\n",
    "total_data[\"gaze_std_x\"] = total_data[\"gaze_std_x\"].bfill()\n",
    "total_data[\"gaze_std_y\"]=total_data[\" gaze_angle_y\"].rolling(10,min_periods=0).std()\n",
    "total_data[\"gaze_std_y\"] = total_data[\"gaze_std_y\"].bfill()\n",
    "total_data = total_data.bfill().ffill()\n",
    "#total_data.to_csv(\"data_coming_back_100L.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-tongue",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------\n",
    "#Preparing the speed data for change point and combining with total_data\n",
    "#-----------------------------------------------------------\n",
    "\n",
    "# You can skip if you have already done the change points \n",
    "\n",
    "speed_0823 = pd.read_csv(\"I:/Phase 1/9/Video/08232020/speed_csv/final_speed.csv\")\n",
    "speed_0823[\"Timestamp\"] = speed_0823['Date'].str.cat(speed_0823['Time'],sep=\"T\")\n",
    "speed_0823[\"Timestamp\"] = pd.to_datetime(speed_0823[\"Timestamp\"], format=\"%m/%d/%YT%H:%M:%S\")\n",
    "speed_0823 = speed_0823.sort_values(by=\"Timestamp\")\n",
    "speed_0823[\"Timestamp\"] = speed_0823[\"Timestamp\"].dt.tz_localize(None)\n",
    "speed_0823[\"Normalized Speed\"] = speed_0823[\"Normalized Speed\"].replace(to_replace=\"not available\",method='bfill') \n",
    "speed_0823[\"Timestamp\"] = speed_0823[\"Timestamp\"] + pd.Timedelta(value=+4,unit='h')\n",
    "start_date = pd.Timestamp(\"2020-08-23T15:16:11.000+0000\",tz=None)\n",
    "end_date = pd.Timestamp(\"2020-08-23T17:16:00.000+0000\",tz=None)\n",
    "start_date = start_date.tz_localize(None)\n",
    "end_date = end_date.tz_localize(None)\n",
    "mask = (speed_0823['Timestamp'] > start_date) & (speed_0823['Timestamp'] <= end_date)\n",
    "speed_0823 = speed_0823.loc[mask]\n",
    "data_for_bcp = total_data.resample('1S').mean().bfill().ffill()\n",
    "data_for_bcp = data_for_bcp.reset_index()\n",
    "\n",
    "\n",
    "data_0823_speed_hr = pd.merge(speed_0823,data_for_bcp,on=[\"Timestamp\"])\n",
    "\n",
    "data_0823_speed_hr.to_csv(\"cleaned_speed_0823.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerical-radical",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------\n",
    "# ran bcp in R. Bring the output here for finding when HR and speed change point co-occur.\n",
    "#-----------------------------------------------------------\n",
    "# Then find the location of interest through peak detection\n",
    "\n",
    "def find_any_peak(segment):    \n",
    "    if ((segment > 0.1)).any():\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "speed_0823_bcp = pd.read_csv(\"cleaned_speed_0823_bcp.csv\",parse_dates=[\"Timestamp\"])\n",
    "\n",
    "\n",
    "speed_0823_bcp[\"Timestamp\"] = speed_0823_bcp[\"Timestamp\"].dt.tz_localize(None)\n",
    "#speed_0823_bcp[\"Timestamp\"] = speed_0823_bcp[\"Timestamp\"] - pd.Timedelta(hours=4)\n",
    "speed_0823_bcp[\"diff\"] = speed_0823_bcp[\"bcp_mean_HR\"].diff()\n",
    "speed_0823_bcp[\"diff\"] = speed_0823_bcp[\"diff\"].shift(-1)\n",
    "speed_0823_bcp[\"type_of_cp\"] = speed_0823_bcp[\"diff\"].apply(lambda x: 1 if x>0 else 0)\n",
    "speed_0823_bcp[\"binary_prob\"] = 0\n",
    "speed_0823_bcp[\"binary_prob\"][(speed_0823_bcp[\"bcp_prob_HR\"]!=0)&(speed_0823_bcp[\"type_of_cp\"]==1)] = 1\n",
    "\n",
    "\n",
    "speed_0823_bcp[\"HR_presence\"] = speed_0823_bcp[\"binary_prob\"].rolling(5).apply(find_any_peak).dropna()\n",
    "speed_0823_bcp[\"speed_presence\"] = speed_0823_bcp[\"bcp_prob_speed\"].rolling(5).apply(find_any_peak).dropna()\n",
    "speed_0823_bcp[\"peak_loc\"] = speed_0823_bcp[\"HR_presence\"] + speed_0823_bcp[\"speed_presence\"]\n",
    "speed_0823_bcp[\"peak_loc\"][speed_0823_bcp[\"peak_loc\"]!=2] =0\n",
    "speed_0823_bcp[\"peak_loc\"][speed_0823_bcp[\"peak_loc\"]==2] =1\n",
    "\n",
    "\n",
    "X = speed_0823_bcp[\"peak_loc\"]\n",
    "peaks, _ = find_peaks(X, distance=60)\n",
    "boolean_obs = np.zeros(X.shape[0])\n",
    "boolean_obs[peaks] = 1\n",
    "speed_0823_bcp[\"boolean_peak\"] = boolean_obs\n",
    "\n",
    "list_of_segments = speed_0823_bcp[\"Timestamp\"][speed_0823_bcp[\"boolean_peak\"]==1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biblical-catering",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------\n",
    "#save for later analysis of bcp of multivariate data\n",
    "#-----------------------------------------------------------\n",
    "\n",
    "total_data.to_csv(\"08232021_with_gmm_for_bcp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desirable-passport",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------\n",
    "# Read the bcp results\n",
    "#-----------------------------------------------------------\n",
    "\n",
    "total_data = pd.read_csv(\"08232021_with_gmm_with_bcp.csv\",parse_dates=[\"Timestamp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medium-growing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------\n",
    "# Add the gaze data\n",
    "#-----------------------------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "max_x = total_data[\"gaze_angle_x\"].max()\n",
    "min_x = total_data[\"gaze_angle_x\"].min()\n",
    "max_y = total_data[\"gaze_angle_y\"].max()\n",
    "min_y = total_data[\"gaze_angle_y\"].min()\n",
    "\n",
    "def linspacing_the_range(x,y,min_x,min_y,max_x,max_y):\n",
    "    XX = np.linspace(min_x, max_x, x)\n",
    "    YY = np.linspace(min_y, max_y, y)\n",
    "    return XX,YY\n",
    "\n",
    "x=4\n",
    "y=4\n",
    "XX,YY = linspacing_the_range(x,y,min_x,min_y,max_x,max_y)\n",
    "\n",
    "counter=0\n",
    "\n",
    "for i in range(x-1):\n",
    "    for j in range(y-1):\n",
    "        total_data.loc[(total_data['gaze_angle_x'] >= XX[i]) & \n",
    "                 (total_data['gaze_angle_x'] <= XX[i+1])& \n",
    "                 (total_data['gaze_angle_y'] >= YY[j])&\n",
    "                  (total_data['gaze_angle_y'] <= YY[j+1]), 'location'] = counter\n",
    "        counter += 1\n",
    "\n",
    "total_data['location'] = total_data['location'].astype(int)\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "def transition_matrix(windowed_data):\n",
    "    transitions = windowed_data.astype(int).values\n",
    "    n = 1+ max(transitions) #number of states\n",
    "\n",
    "    M = [[0]*n for _ in range(n)]\n",
    "\n",
    "    for (i,j) in zip(transitions,transitions[1:]):\n",
    "        M[i][j] += 1\n",
    "\n",
    "    #now convert to probabilities:\n",
    "    for row in M:\n",
    "        s = sum(row)\n",
    "        if s > 0:\n",
    "            row[:] = [f/s for f in row]\n",
    "    m = np.array(M)\n",
    "    m[m==0] = 1/(x*y)\n",
    "    pA = m / m.sum()\n",
    "    Shannon = -np.sum(pA*np.log2(pA))\n",
    "    return Shannon\n",
    "\n",
    "total_data[\"entropy_gaze\"] = total_data[\"location\"].rolling(600).progress_apply(transition_matrix)\n",
    "total_data[\"entropy_gaze\"] = total_data[\"entropy_gaze\"].bfill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "progressive-caution",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------\n",
    "#Exploring how many gmm components we need for transforming data to words - you can skip\n",
    "#-----------------------------------------------------------\n",
    "\n",
    "feats = [\"ZGyro\",\"XLine\",\"YLine\"]\n",
    "X = total_data[feats].values\n",
    "n_components = np.arange(1, 60)\n",
    "models = [GaussianMixture(n, random_state=0).fit(X)\n",
    "          for n in n_components]\n",
    "plt.plot(n_components, [m.bic(X) for m in models], label='BIC')\n",
    "plt.plot(n_components, [m.aic(X) for m in models], label='AIC')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('n_components');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "voluntary-outside",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------\n",
    "# perform gaussian mixture model on the data based on previous section's graph\n",
    "# Accelerometer data\n",
    "#-----------------------------------------------------------\n",
    "\n",
    "\n",
    "feats = [\"ZGyro\",\"XLine\",\"YLine\"]\n",
    "X = total_data[feats].values\n",
    "labels = (GaussianMixture(n_components=50).fit(X)).predict(X)\n",
    "total_data[\"acc_words\"] = labels\n",
    "\n",
    "#let's look at the distributions of GMM components\n",
    "total_data[\"acc_words\"].value_counts().plot(kind='bar')\n",
    "total_data[\"acc_words\"] = total_data[\"acc_words\"]+100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conscious-medication",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform gaussian mixture model on the data based on previous section's graph\n",
    "# Gaze data\n",
    "\n",
    "feats = [\"entropy_gaze\"]\n",
    "X = total_data[feats].values\n",
    "labels = (GaussianMixture(n_components=50).fit(X)).predict(X)\n",
    "total_data[\"gaze_words\"] = labels\n",
    "\n",
    "#let's look at the distributions of GMM components\n",
    "total_data[\"gaze_words\"].value_counts().plot(kind='bar')\n",
    "total_data[\"gaze_words\"] = total_data[\"gaze_words\"]+100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handmade-expert",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------\n",
    "# perform gaussian mixture model on the data based on previous section's graph\n",
    "# HR data\n",
    "#-----------------------------------------------------------\n",
    "\n",
    "feats = [\"HR\"]\n",
    "X = total_data[feats].values\n",
    "labels = (GaussianMixture(n_components=50).fit(X)).predict(X)\n",
    "total_data[\"HR_words\"] = labels\n",
    "\n",
    "#let's look at the distributions of GMM components\n",
    "total_data[\"HR_words\"].value_counts().plot(kind='bar')\n",
    "total_data[\"HR_words\"] = total_data[\"HR_words\"]+100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "toxic-brunswick",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------\n",
    "# find the segments of bcp through peak detection\n",
    "#-----------------------------------------------------------\n",
    "\n",
    "X = total_data[\"segment_bcp_prob\"]\n",
    "peaks, _ = find_peaks(X, distance=50)\n",
    "#sns.distplot(peaks[1:]-peaks[:-1])\n",
    "boolean_obs = np.zeros(X.shape[0])\n",
    "boolean_obs[peaks] = 1\n",
    "total_data[\"boolean_peak\"] = boolean_obs\n",
    "\n",
    "list_of_segments = total_data[\"Timestamp\"][total_data[\"boolean_peak\"]==1].values\n",
    "\n",
    "total_data[\"group\"] = total_data[\"boolean_peak\"].ne(0).cumsum()\n",
    "total_data[\"group\"] = total_data[\"group\"] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hispanic-tanzania",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------\n",
    "#make documents out of segments for all three of acc, gaze, and HR\n",
    "#-----------------------------------------------------------\n",
    "\n",
    "\n",
    "def seg_to_list_acc(seg):\n",
    "    out = ' '.join(seg['acc_words'].astype(str).tolist())\n",
    "    return out\n",
    "docs_acc = total_data.groupby(['group']).apply(seg_to_list_acc).tolist()\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "data_vectorized_acc = vectorizer.fit_transform(docs_acc)\n",
    "\n",
    "\n",
    "def seg_to_list_gaze(seg):\n",
    "    out = ' '.join(seg['gaze_words'].astype(str).tolist())\n",
    "    return out\n",
    "docs_gaze = total_data.groupby(['group']).apply(seg_to_list_gaze).tolist()\n",
    "\n",
    "data_vectorized_gaze = vectorizer.fit_transform(docs_gaze)\n",
    "\n",
    "def seg_to_list_hr(seg):\n",
    "    out = ' '.join(seg['HR_words'].astype(str).tolist())\n",
    "    return out\n",
    "docs_HR = total_data.groupby(['group']).apply(seg_to_list_hr).tolist()\n",
    "\n",
    "data_vectorized_HR = vectorizer.fit_transform(docs_HR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accredited-vision",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------\n",
    "# Build LDA Model based on the previous section for acc\n",
    "#-----------------------------------------------------------\n",
    "\n",
    "\n",
    "lda_model_acc = LatentDirichletAllocation(n_components=4,  \n",
    "                                      max_iter=10000,  \n",
    "                                      learning_method='online',   \n",
    "                                      random_state=100,          \n",
    "                                      batch_size=128,            \n",
    "                                      evaluate_every = -1, \n",
    "                                      n_jobs = -1 )\n",
    "lda_output_acc = lda_model_acc.fit_transform(data_vectorized_acc)\n",
    "\n",
    "# Create Document — Topic Matrix\n",
    "best_lda_model = lda_model_acc\n",
    "#lda_output = best_lda_model.transform(data_vectorized)\n",
    "# column names\n",
    "topicnames = ['Topic' + str(i) for i in range(best_lda_model.n_components)]\n",
    "# index names\n",
    "docnames = ['Doc' + str(i) for i in range(len(docs_acc))]\n",
    "# Make the pandas dataframe\n",
    "df_document_topic = pd.DataFrame(np.round(lda_output_acc, 2), columns=topicnames, index=docnames)\n",
    "# Get dominant topic for each document\n",
    "dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
    "df_document_topic['dominant_topic'] = dominant_topic\n",
    "# Styling\n",
    "def color_green(val):\n",
    " color = 'green' if val > .1 else 'black'\n",
    " return 'color: {col}'.format(col=color)\n",
    "def make_bold(val):\n",
    " weight = 700 if val > .1 else 400\n",
    " return 'font-weight: {weight}'.format(weight=weight)\n",
    "\n",
    "#Save behavioral clusters which are topic of documents\n",
    "\n",
    "total_data[\"behavior_cluster_acc\"] = \"NAN\"\n",
    "for group in total_data[\"group\"].unique():\n",
    "    total_data[\"behavior_cluster_acc\"][total_data[\"group\"]==group] = df_document_topic[\"dominant_topic\"][group-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removable-bible",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------\n",
    "# Build LDA Model based on the previous section for gaze\n",
    "#-----------------------------------------------------------\n",
    "\n",
    "\n",
    "lda_model_gaze = LatentDirichletAllocation(n_components=2,  \n",
    "                                      max_iter=10000,  \n",
    "                                      learning_method='online',   \n",
    "                                      random_state=100,          \n",
    "                                      batch_size=128,            \n",
    "                                      evaluate_every = -1, \n",
    "                                      n_jobs = -1 )\n",
    "lda_output_gaze = lda_model_gaze.fit_transform(data_vectorized_gaze)\n",
    "best_lda_model = lda_model_gaze\n",
    "#lda_output = best_lda_model.transform(data_vectorized)\n",
    "# column names\n",
    "topicnames = ['Topic' + str(i) for i in range(best_lda_model.n_components)]\n",
    "# index names\n",
    "docnames = ['Doc' + str(i) for i in range(len(docs_gaze))]\n",
    "# Make the pandas dataframe\n",
    "df_document_topic = pd.DataFrame(np.round(lda_output_gaze, 2), columns=topicnames, index=docnames)\n",
    "# Get dominant topic for each document\n",
    "dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
    "df_document_topic['dominant_topic'] = dominant_topic\n",
    "# Styling\n",
    "def color_green(val):\n",
    " color = 'green' if val > .1 else 'black'\n",
    " return 'color: {col}'.format(col=color)\n",
    "def make_bold(val):\n",
    " weight = 700 if val > .1 else 400\n",
    " return 'font-weight: {weight}'.format(weight=weight)\n",
    "\n",
    "#Save behavioral clusters which are topic of documents\n",
    "\n",
    "total_data[\"behavior_cluster_gaze\"] = \"NAN\"\n",
    "for group in total_data[\"group\"].unique():\n",
    "    total_data[\"behavior_cluster_gaze\"][total_data[\"group\"]==group] = df_document_topic[\"dominant_topic\"][group-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlled-absorption",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------\n",
    "# Build LDA Model based on the previous section for hr\n",
    "#-----------------------------------------------------------\n",
    "\n",
    "\n",
    "lda_model_HR = LatentDirichletAllocation(n_components=2,  \n",
    "                                      max_iter=10000,  \n",
    "                                      learning_method='online',   \n",
    "                                      random_state=100,          \n",
    "                                      batch_size=128,            \n",
    "                                      evaluate_every = -1, \n",
    "                                      n_jobs = -1 )\n",
    "lda_output_HR = lda_model_HR.fit_transform(data_vectorized_HR)\n",
    "\n",
    "# Create Document — Topic Matrix\n",
    "\n",
    "best_lda_model = lda_model_HR\n",
    "#lda_output = best_lda_model.transform(data_vectorized)\n",
    "# column names\n",
    "topicnames = ['Topic' + str(i) for i in range(best_lda_model.n_components)]\n",
    "# index names\n",
    "docnames = ['Doc' + str(i) for i in range(len(docs_HR))]\n",
    "# Make the pandas dataframe\n",
    "df_document_topic = pd.DataFrame(np.round(lda_output_HR, 2), columns=topicnames, index=docnames)\n",
    "# Get dominant topic for each document\n",
    "dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
    "df_document_topic['dominant_topic'] = dominant_topic\n",
    "# Styling\n",
    "def color_green(val):\n",
    " color = 'green' if val > .1 else 'black'\n",
    " return 'color: {col}'.format(col=color)\n",
    "def make_bold(val):\n",
    " weight = 700 if val > .1 else 400\n",
    " return 'font-weight: {weight}'.format(weight=weight)\n",
    "\n",
    "#Save behavioral clusters which are topic of documents\n",
    "\n",
    "total_data[\"behavior_cluster_hr\"] = \"NAN\"\n",
    "for group in total_data[\"group\"].unique():\n",
    "    total_data[\"behavior_cluster_hr\"][total_data[\"group\"]==group] = df_document_topic[\"dominant_topic\"][group-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "through-small",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------\n",
    "#depending on the number of clusters, save either of them \n",
    "#-----------------------------------------------------------\n",
    "\n",
    "\n",
    "total_data.to_csv(\"08232021_with_gmm_with_bcp_with_lda_2_cats.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caring-cancellation",
   "metadata": {},
   "source": [
    "### Analysis is finished, below is plotting with speed data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "known-recorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------\n",
    "total_data_temp = pd.read_csv(\"08232021_with_gmm_with_bcp_with_lda_4_cats.csv\",parse_dates=[\"Timestamp\"])\n",
    "total_data = pd.read_csv(\"08232021_with_gmm_with_bcp_with_lda_2_cats.csv\",parse_dates=[\"Timestamp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preceding-methodology",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------\n",
    "#combine with 2 cat - just as a temporary item. You can skip later \n",
    "#-------------------------------------------------------------------------------\n",
    "\n",
    "total_data[\"behavior_cluster_acc\"] = total_data_temp[\"behavior_cluster_acc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "residential-columbia",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------\n",
    "#counting the number of normal/abnormal HR for testing in R - note that the HR clusters \n",
    "#are flipped and will be fixed in plotting - road curvature\n",
    "#----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "group_road_curv = total_data[total_data[\"behavior_cluster_acc\"]==2]\n",
    "print(\"abnormal HR: \",group_road_curv.groupby('behavior_cluster_hr')[\"frame\"].count()[0])\n",
    "print(\"normal HR: \",group_road_curv.groupby('behavior_cluster_hr')[\"frame\"].count()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efficient-centre",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------\n",
    "#counting the number of normal/abnormal HR for testing in R - note that the HR clusters \n",
    "#are flipped and will be fixed in plotting - Highway freeflow\n",
    "#----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "group_road_curv = total_data[total_data[\"behavior_cluster_acc\"]==3]\n",
    "print(\"abnormal HR: \",group_road_curv.groupby('behavior_cluster_hr')[\"frame\"].count()[0])\n",
    "print(\"normal HR: \",group_road_curv.groupby('behavior_cluster_hr')[\"frame\"].count()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlled-journalist",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------\n",
    "#counting the number of high/low gaze for testing in R - note that the gaze clusters \n",
    "#are flipped and will be fixed in plotting - Highway freeflow\n",
    "#----------------------------------------------------------------------\n",
    "\n",
    "group_road_curv = total_data[total_data[\"behavior_cluster_acc\"]==3]\n",
    "print(\"high GTE: \",group_road_curv.groupby('behavior_cluster_gaze')[\"frame\"].count()[1])\n",
    "print(\"low GTE: \",group_road_curv.groupby('behavior_cluster_gaze')[\"frame\"].count()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "christian-wales",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------\n",
    "#performing kruskal test for comparing normal and harsh brake - HR\n",
    "#-----------------------------------------------------------\n",
    "\n",
    "group_0 = total_data[total_data[\"behavior_cluster_acc\"]==0]\n",
    "group_1 = total_data[total_data[\"behavior_cluster_acc\"]==1]\n",
    "stats.kruskal(group_0[\"behavior_cluster_hr\"].values,group_1[\"behavior_cluster_hr\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporate-facing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#performing kruskal test for comparing normal and harsh brake - gaze\n",
    "group_0 = total_data[total_data[\"behavior_cluster_acc\"]==0]\n",
    "group_1 = total_data[total_data[\"behavior_cluster_acc\"]==1]\n",
    "stats.kruskal(group_0[\"behavior_cluster_hr\"].values,group_1[\"behavior_cluster_gaze\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "massive-second",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------\n",
    "# combine with the speed data \n",
    "#----------------------------------------------------------------------------------------\n",
    "\n",
    "speed_0823_bcp = pd.read_csv(\"cleaned_speed_0823_bcp.csv\",parse_dates=[\"Timestamp\"])\n",
    "data_for_bcp_2 = total_data.copy()\n",
    "data_for_bcp_2 = data_for_bcp_2.reset_index()\n",
    "feats= [\"behavior_cluster_acc\",\"behavior_cluster_gaze\",\"behavior_cluster_hr\",\"entropy_gaze\",\"Timestamp\",'boolean_peak','segment_bcp_prob']\n",
    "data_for_bcp_2 = data_for_bcp_2[feats]\n",
    "data_for_bcp_2 = data_for_bcp_2.reset_index()\n",
    "speed_0823_bcp = pd.merge(speed_0823_bcp,data_for_bcp_2,on=[\"Timestamp\"])\n",
    "\n",
    "speed_0823_bcp[\"behavior_cluster_acc_int\"] = speed_0823_bcp[\"behavior_cluster_acc\"].astype(int)\n",
    "speed_0823_bcp[\"behavior_cluster_gaze_int\"] = speed_0823_bcp[\"behavior_cluster_gaze\"].astype(int)\n",
    "speed_0823_bcp[\"behavior_cluster_hr_int\"] = speed_0823_bcp[\"behavior_cluster_hr\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-institution",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------\n",
    "# Find statistics of each behavioral category\n",
    "#-----------------------------------------------------------\n",
    "\n",
    "stats = total_data.groupby([\"behavior_cluster_acc\"]).describe()\n",
    "stats_states = speed_0823_bcp.groupby([\"behavior_cluster_acc\"]).describe()\n",
    "stats_states.to_csv(\"stats_behaviors_patterns.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diverse-advisory",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------\n",
    "# Find statistics of each state category\n",
    "#-----------------------------------------------------------\n",
    "\n",
    "\n",
    "stats = total_data.groupby([\"behavior_cluster_hr\"]).describe()\n",
    "stats_states = speed_0823_bcp.groupby([\"behavior_cluster_hr\"]).describe()\n",
    "stats_states.to_csv(\"stats_states_patterns_hr.csv\")\n",
    "\n",
    "stats = total_data.groupby([\"behavior_cluster_gaze\"]).describe()\n",
    "stats_states = speed_0823_bcp.groupby([\"behavior_cluster_gaze\"]).describe()\n",
    "stats_states.to_csv(\"stats_states_patterns_gaze.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behavioral-spotlight",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------\n",
    "# testing differences between distributions\n",
    "#-----------------------------------------------------------\n",
    "\n",
    "\n",
    "beh_0 = speed_0823_bcp[speed_0823_bcp[\"behavior_cluster_hr\"]==0]\n",
    "beh_1 = speed_0823_bcp[speed_0823_bcp[\"behavior_cluster_hr\"]==1]\n",
    "beh_2 = speed_0823_bcp[speed_0823_bcp[\"behavior_cluster_hr\"]==2]\n",
    "beh_3 = speed_0823_bcp[speed_0823_bcp[\"behavior_cluster_hr\"]==3]\n",
    "w,p = kstest(beh_0[\"HR\"],beh_1[\"HR\"])\n",
    "print(w,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expired-fleet",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------\n",
    "#Plotting the fractions of each category of HR and gaze inside driving patterns - calculations\n",
    "#---------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "speed_0823_bcp_temp_for_plot = speed_0823_bcp.copy()\n",
    "speed_0823_bcp_temp_for_plot[\"behavior_cluster_hr\"][speed_0823_bcp_temp_for_plot[\"behavior_cluster_hr\"]==0] = 2\n",
    "speed_0823_bcp_temp_for_plot[\"behavior_cluster_hr\"][speed_0823_bcp_temp_for_plot[\"behavior_cluster_hr\"]==1] = 0\n",
    "speed_0823_bcp_temp_for_plot[\"behavior_cluster_hr\"][speed_0823_bcp_temp_for_plot[\"behavior_cluster_hr\"]==2] = 1\n",
    "\n",
    "counted_data_hr = speed_0823_bcp_temp_for_plot.groupby(['behavior_cluster_acc',\"behavior_cluster_hr\"]).count()[\"frame\"].reset_index()\n",
    "counted_data_gaze = speed_0823_bcp_temp_for_plot.groupby(['behavior_cluster_acc',\"behavior_cluster_gaze\"]).count()[\"frame\"].reset_index()\n",
    "counted_data_hr[\"fraction\"] = counted_data_hr.groupby([\"behavior_cluster_acc\"])[\"frame\"].apply(lambda x: x/x.sum())\n",
    "counted_data_gaze[\"fraction\"] = counted_data_gaze.groupby([\"behavior_cluster_acc\"])[\"frame\"].apply(lambda x: x/x.sum())\n",
    "\n",
    "\n",
    "counted_data_hr[\"behavior_cluster_acc\"][counted_data_hr[\"behavior_cluster_acc\"]==0] = \"Normal Brake\"\n",
    "counted_data_hr[\"behavior_cluster_acc\"][counted_data_hr[\"behavior_cluster_acc\"]==1] = \"Harsh Brake\"\n",
    "counted_data_hr[\"behavior_cluster_acc\"][counted_data_hr[\"behavior_cluster_acc\"]==2] = \"Road Curvature Driving\"\n",
    "counted_data_hr[\"behavior_cluster_acc\"][counted_data_hr[\"behavior_cluster_acc\"]==3] = \"Free Flow Driving\"\n",
    "counted_data_hr[\"behavior_cluster_hr\"][counted_data_hr[\"behavior_cluster_hr\"]==0] = \"Normal Heart Rate\"\n",
    "counted_data_hr[\"behavior_cluster_hr\"][counted_data_hr[\"behavior_cluster_hr\"]==1] = \"Abnormal Heart Rate\"\n",
    "\n",
    "\n",
    "counted_data_gaze[\"behavior_cluster_acc\"][counted_data_gaze[\"behavior_cluster_acc\"]==0] = \"Normal Brake\"\n",
    "counted_data_gaze[\"behavior_cluster_acc\"][counted_data_gaze[\"behavior_cluster_acc\"]==1] = \"Harsh Brake\"\n",
    "counted_data_gaze[\"behavior_cluster_acc\"][counted_data_gaze[\"behavior_cluster_acc\"]==2] = \"Road Curvature Driving\"\n",
    "counted_data_gaze[\"behavior_cluster_acc\"][counted_data_gaze[\"behavior_cluster_acc\"]==3] = \"Free Flow Driving\"\n",
    "counted_data_gaze[\"behavior_cluster_gaze\"][counted_data_gaze[\"behavior_cluster_gaze\"]==0] = \"Low Gaze Entropy\"\n",
    "counted_data_gaze[\"behavior_cluster_gaze\"][counted_data_gaze[\"behavior_cluster_gaze\"]==1] = \"High Gaze Entropy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intellectual-sheriff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------\n",
    "#Plotting the fractions of each category of HR and gaze inside driving patterns - plotting\n",
    "#---------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "fig = px.bar(counted_data_hr, x=\"behavior_cluster_acc\", y=\"fraction\",color=\"behavior_cluster_hr\",pattern_shape=\"behavior_cluster_hr\",pattern_shape_sequence=[\".\", \"x\"])\n",
    "fig.update_layout(  \n",
    "    {    \n",
    "        'yaxis':{'title':\"<b>Fraction of Each Pattern\",'linecolor':'black'},\n",
    "        'xaxis':{'title':\"<b>Behavior Pattern\",'linecolor':'black'},\n",
    "        'height':600,\n",
    "        'width':1000,\n",
    "        'font':dict(size=20),\n",
    "        'paper_bgcolor':'rgba(0,0,0,0)',\n",
    "        'plot_bgcolor':'rgba(0,0,0,0)'\n",
    "        \n",
    "    }\n",
    ")\n",
    "fig.update_layout(legend=dict(\n",
    "    title=\"HR Pattern\",\n",
    "    yanchor=\"top\",\n",
    "    xanchor=\"left\",\n",
    "    orientation=\"h\",\n",
    "    x=0,\n",
    "    y=1.15,\n",
    "    font = dict(size=24)\n",
    "    \n",
    "))\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spoken-sandwich",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------\n",
    "#Plotting the fractions of each category of HR and gaze inside driving patterns - plotting\n",
    "#---------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "fig = px.bar(counted_data_gaze, x=\"behavior_cluster_acc\", y=\"fraction\",color=\"behavior_cluster_gaze\",\n",
    "             pattern_shape=\"behavior_cluster_gaze\",pattern_shape_sequence=[\".\", \"x\"]\n",
    "            ,color_discrete_sequence=[\"red\", \"green\"])\n",
    "fig.update_layout(  \n",
    "    {    \n",
    "        'yaxis':{'title':\"<b>Fraction of Each Pattern\",'linecolor':'black'},\n",
    "        'xaxis':{'title':\"<b>Behavior Pattern\",'linecolor':'black'},\n",
    "        'height':600,\n",
    "        'width':1000,\n",
    "        'font':dict(size=20),\n",
    "        'paper_bgcolor':'rgba(0,0,0,0)',\n",
    "        'plot_bgcolor':'rgba(0,0,0,0)'\n",
    "        \n",
    "    }\n",
    ")\n",
    "fig.update_layout(legend=dict(\n",
    "    title=\"Gaze Pattern\",\n",
    "    yanchor=\"top\",\n",
    "    xanchor=\"left\",\n",
    "    orientation=\"h\",\n",
    "    x=0,\n",
    "    y=1.15,\n",
    "    font = dict(size=24)\n",
    "    \n",
    "))\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "copyrighted-disabled",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------\n",
    "#sample plot for the paper showing clusters and how they relate to real-world data\n",
    "#------------------------------------------------------------------------------------\n",
    "\n",
    "temp_plot = total_data.loc[30000:40000]\n",
    "temp_plot[\"behavior_cluster_acc\"] = pd.Categorical(temp_plot[\"behavior_cluster_acc\"])\n",
    "\n",
    "fig = px.scatter(\n",
    "   x=temp_plot[\"Timestamp\"],y = temp_plot[\"ZGyro\"],color=temp_plot['behavior_cluster_acc']\n",
    ")\n",
    "fig.update_layout(  \n",
    "    {    \n",
    "        'yaxis':{'title':\"<b>Lateral Angular Speed\",'linecolor':'black'},\n",
    "        'xaxis':{'title':\"<b>Time (UTC)\",'linecolor':'black'},\n",
    "        'height':600,\n",
    "        'width':1400,\n",
    "        'font':dict(size=20),\n",
    "        'paper_bgcolor':'rgba(0,0,0,0)',\n",
    "        'plot_bgcolor':'rgba(0,0,0,0)',\n",
    "        'legend_title':'Behavioral Pattern'\n",
    "        \n",
    "    }\n",
    ")\n",
    "fig.update_layout(legend=dict(\n",
    "    yanchor=\"bottom\",\n",
    "    xanchor=\"left\",\n",
    "    orientation=\"h\",\n",
    "    x=0,\n",
    "    y=-0.25\n",
    "    \n",
    "))\n",
    "fig.show()\n",
    "\n",
    "fig = px.scatter(\n",
    "   x=temp_plot[\"Timestamp\"],y = temp_plot[\"YAcce\"],color=temp_plot['behavior_cluster_acc']\n",
    ")\n",
    "fig.update_layout(  \n",
    "    {    \n",
    "        'yaxis':{'title':\"<b>Forward Acceleration\",'linecolor':'black'},\n",
    "        'xaxis':{'title':\"<b>Time (UTC)\",'linecolor':'black'},\n",
    "        'height':600,\n",
    "        'width':1400,\n",
    "        'font':dict(size=20),\n",
    "        'paper_bgcolor':'rgba(0,0,0,0)',\n",
    "        'plot_bgcolor':'rgba(0,0,0,0)',\n",
    "        'legend_title':'Behavioral Pattern'\n",
    "        \n",
    "    }\n",
    ")\n",
    "fig.update_layout(legend=dict(\n",
    "    yanchor=\"bottom\",\n",
    "    xanchor=\"left\",\n",
    "    orientation=\"h\",\n",
    "    x=0,\n",
    "    y=-0.25\n",
    "    \n",
    "))\n",
    "fig.show()\n",
    "\n",
    "fig = px.scatter(\n",
    "   x=temp_plot[\"Timestamp\"],y = temp_plot[\"XAcce\"],color=temp_plot['behavior_cluster_acc']\n",
    ")\n",
    "fig.update_layout(  \n",
    "    {    \n",
    "        'yaxis':{'title':\"<b>Lateral Acceleration\",'linecolor':'black'},\n",
    "        'xaxis':{'title':\"<b>Time (UTC)\",'linecolor':'black'},\n",
    "        'height':600,\n",
    "        'width':1400,\n",
    "        'font':dict(size=20),\n",
    "        'paper_bgcolor':'rgba(0,0,0,0)',\n",
    "        'plot_bgcolor':'rgba(0,0,0,0)',\n",
    "        'legend_title':'Behavioral Pattern'\n",
    "        \n",
    "    }\n",
    ")\n",
    "fig.update_layout(legend=dict(\n",
    "    yanchor=\"bottom\",\n",
    "    xanchor=\"left\",\n",
    "    orientation=\"h\",\n",
    "    x=0,\n",
    "    y=-0.25\n",
    "    \n",
    "))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "introductory-accent",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------\n",
    "#plot state distributions within the acc patterns \n",
    "#------------------------------------------------------------------------------------\n",
    "\n",
    "fig, axes = plt.subplots(1,3,figsize=(28,6))\n",
    "\n",
    "axes[0].set_xlabel(\"Gaze Entropy\",fontsize = 20)\n",
    "axes[0].tick_params(axis='x', labelsize=20)\n",
    "axes[0].set_ylabel(\"Density\",fontsize = 20)\n",
    "axes[0].tick_params(axis='y', labelsize=20)\n",
    "\n",
    "axes[1].set_xlabel(\"HR\",fontsize = 20)\n",
    "axes[1].tick_params(axis='x', labelsize=20)\n",
    "axes[1].set_ylabel(\"Density\",fontsize = 20)\n",
    "axes[1].tick_params(axis='y', labelsize=20)\n",
    "\n",
    "axes[2].set_xlabel(\"Speed\",fontsize = 20)\n",
    "axes[2].tick_params(axis='x', labelsize=20)\n",
    "axes[2].set_ylabel(\"Density\",fontsize = 20)\n",
    "axes[2].tick_params(axis='y', labelsize=20)\n",
    "axes[2].set_xlim([0,125])\n",
    "\n",
    "\n",
    "\n",
    "sns.kdeplot(\n",
    "   data=speed_0823_bcp, x=\"entropy_gaze\", hue=\"behavior_cluster_acc\",ax=axes[0],palette=\"dark\",linewidth=3\n",
    ")\n",
    "axes[0].get_legend().set_title(\"Behavioral Pattern\")\n",
    "\n",
    "sns.kdeplot(\n",
    "   data=speed_0823_bcp, x=\"HR\", hue=\"behavior_cluster_acc\",linewidth=3,\n",
    "  ax=axes[1],palette=\"dark\"\n",
    ")\n",
    "axes[1].get_legend().set_title(\"Behavioral Pattern\")\n",
    "\n",
    "sns.kdeplot(\n",
    "   data=speed_0823_bcp, x=\"Normalized.Speed\", hue=\"behavior_cluster_acc\",linewidth=3,\n",
    "   ax=axes[2],palette=\"dark\"\n",
    ")\n",
    "axes[2].get_legend().set_title(\"Behavioral Pattern\")\n",
    "\n",
    "#fig.savefig(\"distribution_1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premier-steel",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------\n",
    "#plot kinematic distributions within the acc patterns \n",
    "#------------------------------------------------------------------------------------\n",
    "\n",
    "fig, axes = plt.subplots(1,3,figsize=(28,6))\n",
    "\n",
    "axes[0].set_xlabel(\"Forward Acceleration\",fontsize = 20)\n",
    "axes[0].tick_params(axis='x', labelsize=20)\n",
    "axes[0].set_ylabel(\"Density\",fontsize = 20)\n",
    "axes[0].tick_params(axis='y', labelsize=20)\n",
    "axes[0].set_xlim([-1.5,1.5])\n",
    "\n",
    "axes[1].set_xlabel(\"Lateral Acceleration\",fontsize = 20)\n",
    "axes[1].tick_params(axis='x', labelsize=20)\n",
    "axes[1].set_ylabel(\"Density\",fontsize = 20)\n",
    "axes[1].tick_params(axis='y', labelsize=20)\n",
    "axes[1].set_xlim([-1.5,1.5])\n",
    "\n",
    "axes[2].set_xlabel(\"Lateral Angular Speed\",fontsize = 20)\n",
    "axes[2].tick_params(axis='x', labelsize=20)\n",
    "axes[2].set_ylabel(\"Density\",fontsize = 20)\n",
    "axes[2].tick_params(axis='y', labelsize=20)\n",
    "axes[2].set_xlim([-0.055,0.055])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sns.kdeplot(\n",
    "   data=speed_0823_bcp, x=\"YAcce\", hue=\"behavior_cluster_acc\",\n",
    "   linewidth=5,ax=axes[0],palette=\"Spectral\"\n",
    ")\n",
    "axes[0].get_legend().set_title(\"Behavioral Pattern\")\n",
    "\n",
    "sns.kdeplot(\n",
    "   data=speed_0823_bcp, x=\"XAcce\", hue=\"behavior_cluster_acc\",\n",
    "   linewidth=5,ax=axes[1],palette=\"Spectral\"\n",
    ")\n",
    "axes[1].get_legend().set_title(\"Behavioral Pattern\")\n",
    "\n",
    "sns.kdeplot(\n",
    "   data=speed_0823_bcp, x=\"ZGyro\", hue=\"behavior_cluster_acc\",\n",
    "   linewidth=5,ax=axes[2],palette=\"Spectral\"\n",
    ")\n",
    "axes[2].get_legend().set_title(\"Behavioral Pattern\")\n",
    "\n",
    "fig.savefig(\"distribution_2.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wicked-egypt",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------\n",
    "#plot all distributions during the acc patterns \n",
    "#------------------------------------------------------------------------------------\n",
    "\n",
    "speed_0823_bcp_temp_for_plot = speed_0823_bcp.copy()\n",
    "speed_0823_bcp_temp_for_plot[\"behavior_cluster_hr\"][speed_0823_bcp_temp_for_plot[\"behavior_cluster_hr\"]==0] = 2\n",
    "speed_0823_bcp_temp_for_plot[\"behavior_cluster_hr\"][speed_0823_bcp_temp_for_plot[\"behavior_cluster_hr\"]==1] = 0\n",
    "speed_0823_bcp_temp_for_plot[\"behavior_cluster_hr\"][speed_0823_bcp_temp_for_plot[\"behavior_cluster_hr\"]==2] = 1\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1,3,figsize=(28,6))\n",
    "\n",
    "axes[0].set_xlabel(\"Gaze Entropy\",fontsize = 20)\n",
    "axes[0].tick_params(axis='x', labelsize=20)\n",
    "axes[0].set_ylabel(\"Density\",fontsize = 20)\n",
    "axes[0].tick_params(axis='y', labelsize=20)\n",
    "\n",
    "axes[1].set_xlabel(\"HR\",fontsize = 20)\n",
    "axes[1].tick_params(axis='x', labelsize=20)\n",
    "axes[1].set_ylabel(\"Density\",fontsize = 20)\n",
    "axes[1].tick_params(axis='y', labelsize=20)\n",
    "\n",
    "axes[2].set_xlabel(\"Speed\",fontsize = 20)\n",
    "axes[2].tick_params(axis='x', labelsize=20)\n",
    "axes[2].set_ylabel(\"Density\",fontsize = 20)\n",
    "axes[2].tick_params(axis='y', labelsize=20)\n",
    "axes[2].set_xlim([0,125])\n",
    "\n",
    "\n",
    "\n",
    "sns.kdeplot(\n",
    "   data=speed_0823_bcp_temp_for_plot, x=\"entropy_gaze\", hue=\"behavior_cluster_gaze\",ax=axes[0],palette=\"dark\",linewidth=3\n",
    ")\n",
    "axes[0].get_legend().set_title(\"Gaze Entropy Pattern\")\n",
    "\n",
    "sns.kdeplot(\n",
    "   data=speed_0823_bcp_temp_for_plot, x=\"HR\", hue=\"behavior_cluster_hr\",linewidth=3,\n",
    "  ax=axes[1],palette=\"dark\"\n",
    ")\n",
    "axes[1].get_legend().set_title(\"HR Pattern\")\n",
    "\n",
    "sns.kdeplot(\n",
    "   data=speed_0823_bcp_temp_for_plot, x=\"Normalized.Speed\", hue=\"behavior_cluster_hr\",linewidth=3,\n",
    "   ax=axes[2],palette=\"dark\"\n",
    ")\n",
    "axes[2].get_legend().set_title(\"Behavioral Pattern\")\n",
    "\n",
    "fig.savefig(\"HR_gaze_cluster_2.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secret-swing",
   "metadata": {},
   "outputs": [],
   "source": [
    "counted_data_hr = speed_0823_bcp.groupby(['behavior_cluster_acc',\"behavior_cluster_hr\"]).count()[\"X\"].reset_index()\n",
    "counted_data_gaze = speed_0823_bcp.groupby(['behavior_cluster_acc',\"behavior_cluster_gaze\"]).count()[\"X\"].reset_index()\n",
    "\n",
    "\n",
    "counted_data_hr[\"behavior_cluster_acc\"][counted_data_hr[\"behavior_cluster_acc\"]==0] = \"Normal Breaking\"\n",
    "counted_data_hr[\"behavior_cluster_acc\"][counted_data_hr[\"behavior_cluster_acc\"]==1] = \"Harsh Breaking\"\n",
    "counted_data_hr[\"behavior_cluster_acc\"][counted_data_hr[\"behavior_cluster_acc\"]==2] = \"Curve Driving\"\n",
    "counted_data_hr[\"behavior_cluster_acc\"][counted_data_hr[\"behavior_cluster_acc\"]==3] = \"Free Flow Driving\"\n",
    "counted_data_hr[\"behavior_cluster_hr\"][counted_data_hr[\"behavior_cluster_hr\"]==0] = \"Abnormal Heart Rate\"\n",
    "counted_data_hr[\"behavior_cluster_hr\"][counted_data_hr[\"behavior_cluster_hr\"]==1] = \"Normal Heart Rate\"\n",
    "\n",
    "\n",
    "counted_data_gaze[\"behavior_cluster_acc\"][counted_data_gaze[\"behavior_cluster_acc\"]==0] = \"Normal Breaking\"\n",
    "counted_data_gaze[\"behavior_cluster_acc\"][counted_data_gaze[\"behavior_cluster_acc\"]==1] = \"Harsh Breaking\"\n",
    "counted_data_gaze[\"behavior_cluster_acc\"][counted_data_gaze[\"behavior_cluster_acc\"]==2] = \"Curve Driving\"\n",
    "counted_data_gaze[\"behavior_cluster_acc\"][counted_data_gaze[\"behavior_cluster_acc\"]==3] = \"Free Flow Driving\"\n",
    "counted_data_gaze[\"behavior_cluster_gaze\"][counted_data_gaze[\"behavior_cluster_gaze\"]==0] = \"Low Gaze Entropy\"\n",
    "counted_data_gaze[\"behavior_cluster_gaze\"][counted_data_gaze[\"behavior_cluster_gaze\"]==1] = \"High Gaze Entropy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hydraulic-benjamin",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------\n",
    "#Plot the behavioral clusters at each segments\n",
    "#---------------------------------------------------------\n",
    "\n",
    "#specific packages needed\n",
    "#------------------------------------------------------------\n",
    "\n",
    "import matplotlib as mpl\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "speed_0823_bcp = speed_0823_bcp.set_index(\"Timestamp\")\n",
    "fig = plt.figure()\n",
    "ax = speed_0823_bcp['Normalized.Speed'].plot()\n",
    "\n",
    "def set_size(w,h, ax=None):\n",
    "    \"\"\" w, h: width, height in inches \"\"\"\n",
    "    if not ax: ax=plt.gca()\n",
    "    l = ax.figure.subplotpars.left\n",
    "    r = ax.figure.subplotpars.right\n",
    "    t = ax.figure.subplotpars.top\n",
    "    b = ax.figure.subplotpars.bottom\n",
    "    figw = float(w)/(r-l)\n",
    "    figh = float(h)/(t-b)\n",
    "    ax.figure.set_size_inches(figw, figh)\n",
    "#set_size(5,5)\n",
    "plt.rcParams['font.size'] = '15'\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "ax.figure.set_size_inches(19, 8)\n",
    "ax.set_xlabel(\"Time\",fontsize=25)\n",
    "ax.set_ylabel(\"Speed(km/h)\",fontsize=25)\n",
    "ax.get_lines()[0].set_color(\"black\")\n",
    "\n",
    "cmap = ListedColormap(['gray', 'yellow','red','blue'])\n",
    "\n",
    "z = ax.pcolorfast(ax.get_xlim(), ax.get_ylim(),\n",
    "              speed_0823_bcp['behavior_cluster_acc_int'].values[np.newaxis],\n",
    "              cmap=cmap,alpha=0.3)\n",
    "plt.colorbar(z,ticks=[0,1,2,3,4])\n",
    "\n",
    "fig.savefig(\"acc.png\")\n",
    "\n",
    "fig=plt.figure()\n",
    "ax = speed_0823_bcp['Normalized.Speed'].plot()\n",
    "def set_size(w,h, ax=None):\n",
    "    \"\"\" w, h: width, height in inches \"\"\"\n",
    "    if not ax: ax=plt.gca()\n",
    "    l = ax.figure.subplotpars.left\n",
    "    r = ax.figure.subplotpars.right\n",
    "    t = ax.figure.subplotpars.top\n",
    "    b = ax.figure.subplotpars.bottom\n",
    "    figw = float(w)/(r-l)\n",
    "    figh = float(h)/(t-b)\n",
    "    ax.figure.set_size_inches(figw, figh)\n",
    "#set_size(5,5)\n",
    "plt.rcParams['font.size'] = '15'\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "ax.figure.set_size_inches(19, 8)\n",
    "ax.set_xlabel(\"Time\",fontsize=25)\n",
    "ax.set_ylabel(\"Speed(km/h)\",fontsize=25)\n",
    "ax.get_lines()[0].set_color(\"black\")\n",
    "\n",
    "cmap = ListedColormap(['gray','yellow'])\n",
    "z = ax.pcolorfast(ax.get_xlim(), ax.get_ylim(),\n",
    "              speed_0823_bcp['behavior_cluster_gaze_int'].values[np.newaxis],\n",
    "              cmap=cmap, alpha=0.9)\n",
    "plt.colorbar(z,ticks=[0,1])\n",
    "\n",
    "fig.savefig(\"gaze.png\")\n",
    "\n",
    "fig=plt.figure()\n",
    "ax = speed_0823_bcp['Normalized.Speed'].plot()\n",
    "def set_size(w,h, ax=None):\n",
    "    \"\"\" w, h: width, height in inches \"\"\"\n",
    "    if not ax: ax=plt.gca()\n",
    "    l = ax.figure.subplotpars.left\n",
    "    r = ax.figure.subplotpars.right\n",
    "    t = ax.figure.subplotpars.top\n",
    "    b = ax.figure.subplotpars.bottom\n",
    "    figw = float(w)/(r-l)\n",
    "    figh = float(h)/(t-b)\n",
    "    ax.figure.set_size_inches(figw, figh)\n",
    "#set_size(5,5)\n",
    "plt.rcParams['font.size'] = '15'\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "ax.figure.set_size_inches(19, 8)\n",
    "ax.set_xlabel(\"Time\",fontsize=25)\n",
    "ax.set_ylabel(\"Speed(km/h)\",fontsize=25)\n",
    "ax.get_lines()[0].set_color(\"black\")\n",
    "\n",
    "cmap = ListedColormap(['red','blue'])\n",
    "z = ax.pcolorfast(ax.get_xlim(), ax.get_ylim(),\n",
    "              speed_0823_bcp['behavior_cluster_hr_int'].values[np.newaxis],\n",
    "              cmap=cmap, alpha=0.3)\n",
    "plt.colorbar(z,ticks=[0,1,2,3,4])\n",
    "\n",
    "fig.savefig(\"hr.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closed-geology",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------\n",
    "#Plot different segmentations for showcasing how segmentation works in the paper\n",
    "#---------------------------------------------------------\n",
    "\n",
    "# plotly line figure\n",
    "fig = px.line(speed_0823_bcp, x=speed_0823_bcp[\"Timestamp\"], y='Normalized.Speed')\n",
    "\n",
    "# lines to add, specified by x-position\n",
    "lines = list(speed_0823_bcp[speed_0823_bcp[\"segment_bcp_prob\"]>0][\"Timestamp\"])\n",
    "\n",
    "# add lines using absolute references\n",
    "for k in range(len(lines)):\n",
    "    #print(k)\n",
    "    fig.add_shape(type='line',\n",
    "                yref=\"y\",\n",
    "                xref=\"x\",\n",
    "                x0=lines[k],\n",
    "                y0=speed_0823_bcp['Normalized.Speed'].min()*1.2,\n",
    "                x1=lines[k],\n",
    "                y1=speed_0823_bcp['Normalized.Speed'].max()*1.2,\n",
    "                line=dict(color='red', width=2,dash='dot'))\n",
    "\n",
    "\n",
    "    \n",
    "fig.update_layout(  \n",
    "    {    \n",
    "        'yaxis':{'title':\"<b>Speed (km/h)\",'linecolor':'black'},\n",
    "        'xaxis':{'title':\"<b>Time (UTC)\",'linecolor':'black'},\n",
    "        'height':600,\n",
    "        'width':1000,\n",
    "        'font':dict(size=20),\n",
    "        'paper_bgcolor':'rgba(0,0,0,0)',\n",
    "        'plot_bgcolor':'rgba(0,0,0,0)'\n",
    "        \n",
    "    }\n",
    ")\n",
    "fig.update_layout(legend=dict(\n",
    "    yanchor=\"top\",\n",
    "    xanchor=\"left\",\n",
    "    \n",
    "))\n",
    "fig.update_traces(line_color='black')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contemporary-yield",
   "metadata": {},
   "source": [
    "### Calculate transition matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specific-jefferson",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------\n",
    "list_of_segments = total_data[\"Timestamp\"][total_data[\"boolean_peak\"]==1].values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
